{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUYwKUVrIoag",
    "outputId": "57180365-8e1b-4da1-94ed-c4c0ab0bc62a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rasterio\n",
      "  Obtaining dependency information for rasterio from https://files.pythonhosted.org/packages/09/b9/169a76e257e527d352da021da6602480a829eac03b0ab3045639c3f80fb6/rasterio-1.4.2-cp311-cp311-macosx_14_0_arm64.whl.metadata\n",
      "  Downloading rasterio-1.4.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting affine (from rasterio)\n",
      "  Obtaining dependency information for affine from https://files.pythonhosted.org/packages/0b/f7/85273299ab57117850cc0a936c64151171fac4da49bc6fba0dad984a7c5f/affine-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs in /Users/pgt/anaconda3/lib/python3.11/site-packages (from rasterio) (22.1.0)\n",
      "Requirement already satisfied: certifi in /Users/pgt/anaconda3/lib/python3.11/site-packages (from rasterio) (2023.11.17)\n",
      "Requirement already satisfied: click>=4.0 in /Users/pgt/anaconda3/lib/python3.11/site-packages (from rasterio) (8.0.4)\n",
      "Collecting cligj>=0.5 (from rasterio)\n",
      "  Obtaining dependency information for cligj>=0.5 from https://files.pythonhosted.org/packages/73/86/43fa9f15c5b9fb6e82620428827cd3c284aa933431405d1bcf5231ae3d3e/cligj-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy>=1.24 in /Users/pgt/anaconda3/lib/python3.11/site-packages (from rasterio) (1.24.3)\n",
      "Collecting click-plugins (from rasterio)\n",
      "  Obtaining dependency information for click-plugins from https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pyparsing in /Users/pgt/anaconda3/lib/python3.11/site-packages (from rasterio) (3.0.9)\n",
      "Downloading rasterio-1.4.2-cp311-cp311-macosx_14_0_arm64.whl (18.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.8/18.8 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
      "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.2\n",
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/d0/db/5d9cbfbc7968d79c5c09a0bc0bc3735da079f2fd07cc10498a62b320a480/torch-2.5.1-cp311-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torch-2.5.1-cp311-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Obtaining dependency information for typing-extensions>=4.8.0 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: networkx in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Obtaining dependency information for sympy==1.13.1 from https://files.pythonhosted.org/packages/b2/fe/81695a1aa331a842b582453b605175f419fe8540355886031328089d840a/sympy-1.13.1-py3-none-any.whl.metadata\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pgt/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pgt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Downloading torch-2.5.1-cp311-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, sympy, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.11.1\n",
      "    Uninstalling sympy-1.11.1:\n",
      "      Successfully uninstalled sympy-1.11.1\n",
      "Successfully installed sympy-1.13.1 torch-2.5.1 typing-extensions-4.12.2\n",
      "Collecting torchvision\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/28/57/4d7ad90be612f5ac6c4bdafcb0ff13e818e14a340a88c8ca00d9ed8c2dad/torchvision-0.20.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torchvision-0.20.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: torch==2.5.1 in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torchvision) (2.5.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: filelock in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (2023.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/pgt/anaconda3/lib/python3.11/site-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pgt/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pgt/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.5.1->torchvision) (2.1.1)\n",
      "Downloading torchvision-0.20.1-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.20.1\n",
      "Collecting torchsummary\n",
      "  Obtaining dependency information for torchsummary from https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl.metadata\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rasterio\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jSp6UQf7JB5R"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import rasterio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "m1zZ2oH-KjmF"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SEN12MSDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to the dataset folder containing image files.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [\n",
    "            f for f in os.listdir(root_dir) if f.endswith('.tif') or f.endswith('.TIF')\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        with rasterio.open(img_path) as src:\n",
    "            data = src.read()  # Load all bands\n",
    "            rgb = np.stack([data[3], data[2], data[1]], axis=-1)  # Red, Green, Blue (1-based indexing)\n",
    "            nir = data[7]  # Near-Infrared band\n",
    "\n",
    "        # Normalize bands to [0, 1] range\n",
    "        rgb = rgb / 10000.0\n",
    "        nir = nir / 10000.0\n",
    "\n",
    "        # Apply separate transformations for RGB and NIR\n",
    "        if self.transform:\n",
    "            rgb = self.transform(rgb)\n",
    "            nir = transforms.ToPILImage()(nir)\n",
    "            nir = transforms.Resize((256, 256))(nir)\n",
    "            nir = transforms.ToTensor()(nir)\n",
    "              # Add channel dimension\n",
    "\n",
    "        return rgb, nir  # Convert NIR to tensor directly\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "root_dir = \"/Users/pgt/Downloads/Sentinel train data\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = SEN12MSDataset(root_dir=root_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "aFt8SsJ9F37Z",
    "outputId": "3ae4dee1-9c26-4f1c-ea57-a25cd4615d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 17\n",
      "Train images: 13\n",
      "Test images: 4\n"
     ]
    }
   ],
   "source": [
    "#execute this cell when you have local dataset\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(source_dir, train_dir, test_dir, test_size=0.2, random_state=42):\n",
    "    # Create train and test directories if they don't exist\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(source_dir) if f.endswith(('.tif', '.TIF'))]\n",
    "\n",
    "    # Split files\n",
    "    train_files, test_files = train_test_split(\n",
    "        image_files,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Copy train files\n",
    "    for file in train_files:\n",
    "        src_path = os.path.join(source_dir, file)\n",
    "        dst_path = os.path.join(train_dir, file)\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    # Copy test files\n",
    "    for file in test_files:\n",
    "        src_path = os.path.join(source_dir, file)\n",
    "        dst_path = os.path.join(test_dir, file)\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    print(f\"Total images: {len(image_files)}\")\n",
    "    print(f\"Train images: {len(train_files)}\")\n",
    "    print(f\"Test images: {len(test_files)}\")\n",
    "\n",
    "# Example usage\n",
    "source_dir = \"/Users/pgt/Downloads/Sentinel train data\"\n",
    "train_dir = \"/Users/pgt/Downloads/Sentinel train data/train\"\n",
    "test_dir = \"/Users/pgt/Downloads/Sentinel train data/test\"\n",
    "\n",
    "split_dataset(source_dir, train_dir, test_dir)\n",
    "\n",
    "train_dataset = SEN12MSDataset(root_dir=train_dir, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rqv8JzNtXP1a"
   },
   "outputs": [],
   "source": [
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "\n",
    "        def down_block(in_channels, out_channels, normalize=True):\n",
    "            layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        def up_block(in_channels, out_channels, dropout=False):\n",
    "            layers = [\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(0.5))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            down_block(3, 64, normalize=False),\n",
    "            down_block(64, 128),\n",
    "            down_block(128, 256),\n",
    "            down_block(256, 512),\n",
    "            down_block(512, 512),\n",
    "            down_block(512, 512),\n",
    "            down_block(512, 512)\n",
    "        ])\n",
    "\n",
    "        self.decoder = nn.ModuleList([\n",
    "            up_block(512, 512, dropout=True),\n",
    "            up_block(1024, 512, dropout=True),\n",
    "            up_block(1024, 512, dropout=True),\n",
    "            up_block(1024, 256),\n",
    "            up_block(512, 128),\n",
    "            up_block(256, 64),\n",
    "            up_block(128, out_channels)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is the correct size\n",
    "        x = F.interpolate(x, size=(256, 256), mode='bilinear', align_corners=True)\n",
    "\n",
    "        skips = []\n",
    "        # Encoder path\n",
    "        for down in self.encoder:\n",
    "            x = down(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        skips = skips[:-1][::-1]  # Reversing to align with decoder\n",
    "\n",
    "        # Decoder path\n",
    "        for idx, up in enumerate(self.decoder):\n",
    "            x = up(x)\n",
    "\n",
    "            if idx < len(skips):\n",
    "                # Resize decoder output to match skip connection dimensions\n",
    "                x = F.interpolate(x, size=(skips[idx].shape[2], skips[idx].shape[3]), mode='bilinear', align_corners=True)\n",
    "\n",
    "                # Concatenate with skip connection\n",
    "                x = torch.cat((x, skips[idx]), dim=1)\n",
    "\n",
    "        return x  # Remove torch.tanh() to keep original dynamic range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "u63a5tkRXX12",
    "outputId": "a4922730-8426-4353-faae-12ac20b24137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/2] [D loss: 0.7590886354446411] [G loss: 42.422401428222656]\n",
      "[Epoch 0/200] [Batch 1/2] [D loss: 0.4605754315853119] [G loss: 49.69865417480469]\n",
      "[Epoch 1/200] [Batch 0/2] [D loss: 0.8940381407737732] [G loss: 30.385292053222656]\n",
      "[Epoch 1/200] [Batch 1/2] [D loss: 2.0108044147491455] [G loss: 33.72285461425781]\n",
      "[Epoch 2/200] [Batch 0/2] [D loss: 1.8384779691696167] [G loss: 22.906776428222656]\n",
      "[Epoch 2/200] [Batch 1/2] [D loss: 2.559666633605957] [G loss: 30.818586349487305]\n",
      "[Epoch 3/200] [Batch 0/2] [D loss: 1.7806153297424316] [G loss: 19.144041061401367]\n",
      "[Epoch 3/200] [Batch 1/2] [D loss: 1.5153573751449585] [G loss: 25.133817672729492]\n",
      "[Epoch 4/200] [Batch 0/2] [D loss: 1.130022644996643] [G loss: 15.114501953125]\n",
      "[Epoch 4/200] [Batch 1/2] [D loss: 0.40101975202560425] [G loss: 24.463926315307617]\n",
      "[Epoch 5/200] [Batch 0/2] [D loss: 0.4148412048816681] [G loss: 13.188583374023438]\n",
      "[Epoch 5/200] [Batch 1/2] [D loss: 0.30613285303115845] [G loss: 17.835960388183594]\n",
      "[Epoch 6/200] [Batch 0/2] [D loss: 0.34518134593963623] [G loss: 13.749059677124023]\n",
      "[Epoch 6/200] [Batch 1/2] [D loss: 0.2944093942642212] [G loss: 18.47459602355957]\n",
      "[Epoch 7/200] [Batch 0/2] [D loss: 0.2683241367340088] [G loss: 13.100341796875]\n",
      "[Epoch 7/200] [Batch 1/2] [D loss: 0.23327121138572693] [G loss: 18.243392944335938]\n",
      "[Epoch 8/200] [Batch 0/2] [D loss: 0.28411000967025757] [G loss: 11.31300163269043]\n",
      "[Epoch 8/200] [Batch 1/2] [D loss: 0.2565293610095978] [G loss: 18.385068893432617]\n",
      "[Epoch 9/200] [Batch 0/2] [D loss: 0.28610214591026306] [G loss: 15.178789138793945]\n",
      "[Epoch 9/200] [Batch 1/2] [D loss: 0.2531084716320038] [G loss: 14.64565372467041]\n",
      "[Epoch 10/200] [Batch 0/2] [D loss: 0.2702186107635498] [G loss: 10.926512718200684]\n",
      "[Epoch 10/200] [Batch 1/2] [D loss: 0.18692637979984283] [G loss: 17.23884391784668]\n",
      "[Epoch 11/200] [Batch 0/2] [D loss: 0.2613585889339447] [G loss: 12.106853485107422]\n",
      "[Epoch 11/200] [Batch 1/2] [D loss: 0.28047657012939453] [G loss: 15.226299285888672]\n",
      "[Epoch 12/200] [Batch 0/2] [D loss: 0.25259536504745483] [G loss: 9.844963073730469]\n",
      "[Epoch 12/200] [Batch 1/2] [D loss: 0.22817362844944] [G loss: 12.456269264221191]\n",
      "[Epoch 13/200] [Batch 0/2] [D loss: 0.2463262379169464] [G loss: 9.17383861541748]\n",
      "[Epoch 13/200] [Batch 1/2] [D loss: 0.23026621341705322] [G loss: 13.27579116821289]\n",
      "[Epoch 14/200] [Batch 0/2] [D loss: 0.2420346736907959] [G loss: 9.010790824890137]\n",
      "[Epoch 14/200] [Batch 1/2] [D loss: 0.24401171505451202] [G loss: 11.306674003601074]\n",
      "[Epoch 15/200] [Batch 0/2] [D loss: 0.24114933609962463] [G loss: 9.898120880126953]\n",
      "[Epoch 15/200] [Batch 1/2] [D loss: 0.2643381357192993] [G loss: 13.625443458557129]\n",
      "[Epoch 16/200] [Batch 0/2] [D loss: 0.23544941842556] [G loss: 9.794882774353027]\n",
      "[Epoch 16/200] [Batch 1/2] [D loss: 0.263888955116272] [G loss: 18.402395248413086]\n",
      "[Epoch 17/200] [Batch 0/2] [D loss: 0.2333454042673111] [G loss: 8.617319107055664]\n",
      "[Epoch 17/200] [Batch 1/2] [D loss: 0.2688455581665039] [G loss: 16.19669532775879]\n",
      "[Epoch 18/200] [Batch 0/2] [D loss: 0.23278170824050903] [G loss: 8.839507102966309]\n",
      "[Epoch 18/200] [Batch 1/2] [D loss: 0.24242185056209564] [G loss: 14.730133056640625]\n",
      "[Epoch 19/200] [Batch 0/2] [D loss: 0.21524006128311157] [G loss: 9.77832317352295]\n",
      "[Epoch 19/200] [Batch 1/2] [D loss: 0.22891223430633545] [G loss: 15.948223114013672]\n",
      "[Epoch 20/200] [Batch 0/2] [D loss: 0.20719075202941895] [G loss: 8.2772216796875]\n",
      "[Epoch 20/200] [Batch 1/2] [D loss: 0.1961466521024704] [G loss: 14.167925834655762]\n",
      "[Epoch 21/200] [Batch 0/2] [D loss: 0.20014694333076477] [G loss: 8.106585502624512]\n",
      "[Epoch 21/200] [Batch 1/2] [D loss: 0.15691879391670227] [G loss: 12.95645523071289]\n",
      "[Epoch 22/200] [Batch 0/2] [D loss: 0.19432485103607178] [G loss: 9.158759117126465]\n",
      "[Epoch 22/200] [Batch 1/2] [D loss: 0.21424002945423126] [G loss: 13.733574867248535]\n",
      "[Epoch 23/200] [Batch 0/2] [D loss: 0.19449694454669952] [G loss: 8.356072425842285]\n",
      "[Epoch 23/200] [Batch 1/2] [D loss: 0.20599573850631714] [G loss: 11.380766868591309]\n",
      "[Epoch 24/200] [Batch 0/2] [D loss: 0.18695321679115295] [G loss: 8.514066696166992]\n",
      "[Epoch 24/200] [Batch 1/2] [D loss: 0.229007750749588] [G loss: 19.227214813232422]\n",
      "[Epoch 25/200] [Batch 0/2] [D loss: 0.2974749803543091] [G loss: 8.3848876953125]\n",
      "[Epoch 25/200] [Batch 1/2] [D loss: 0.23071295022964478] [G loss: 12.836592674255371]\n",
      "[Epoch 26/200] [Batch 0/2] [D loss: 0.3054302930831909] [G loss: 7.81697940826416]\n",
      "[Epoch 26/200] [Batch 1/2] [D loss: 0.2067808210849762] [G loss: 13.608878135681152]\n",
      "[Epoch 27/200] [Batch 0/2] [D loss: 0.37294802069664] [G loss: 7.635483264923096]\n",
      "[Epoch 27/200] [Batch 1/2] [D loss: 0.2327895313501358] [G loss: 12.04656982421875]\n",
      "[Epoch 28/200] [Batch 0/2] [D loss: 0.3601756691932678] [G loss: 7.722999572753906]\n",
      "[Epoch 28/200] [Batch 1/2] [D loss: 0.22571520507335663] [G loss: 9.732357025146484]\n",
      "[Epoch 29/200] [Batch 0/2] [D loss: 0.3184567093849182] [G loss: 8.38598346710205]\n",
      "[Epoch 29/200] [Batch 1/2] [D loss: 0.19611719250679016] [G loss: 12.253800392150879]\n",
      "[Epoch 30/200] [Batch 0/2] [D loss: 0.3029586672782898] [G loss: 7.8880815505981445]\n",
      "[Epoch 30/200] [Batch 1/2] [D loss: 0.1808568388223648] [G loss: 10.448773384094238]\n",
      "[Epoch 31/200] [Batch 0/2] [D loss: 0.19356870651245117] [G loss: 8.25635814666748]\n",
      "[Epoch 31/200] [Batch 1/2] [D loss: 0.3078875243663788] [G loss: 11.343735694885254]\n",
      "[Epoch 32/200] [Batch 0/2] [D loss: 0.16483838856220245] [G loss: 8.15710735321045]\n",
      "[Epoch 32/200] [Batch 1/2] [D loss: 0.10857269167900085] [G loss: 17.664749145507812]\n",
      "[Epoch 33/200] [Batch 0/2] [D loss: 0.15519776940345764] [G loss: 8.66406536102295]\n",
      "[Epoch 33/200] [Batch 1/2] [D loss: 0.1662984937429428] [G loss: 16.058950424194336]\n",
      "[Epoch 34/200] [Batch 0/2] [D loss: 0.18625304102897644] [G loss: 8.291831970214844]\n",
      "[Epoch 34/200] [Batch 1/2] [D loss: 0.16074129939079285] [G loss: 13.239899635314941]\n",
      "[Epoch 35/200] [Batch 0/2] [D loss: 0.18150171637535095] [G loss: 7.608829021453857]\n",
      "[Epoch 35/200] [Batch 1/2] [D loss: 0.08098438382148743] [G loss: 11.541831016540527]\n",
      "[Epoch 36/200] [Batch 0/2] [D loss: 0.110205739736557] [G loss: 8.178372383117676]\n",
      "[Epoch 36/200] [Batch 1/2] [D loss: 0.2101050317287445] [G loss: 15.375274658203125]\n",
      "[Epoch 37/200] [Batch 0/2] [D loss: 0.111832395195961] [G loss: 8.185644149780273]\n",
      "[Epoch 37/200] [Batch 1/2] [D loss: 0.3353765606880188] [G loss: 13.506345748901367]\n",
      "[Epoch 38/200] [Batch 0/2] [D loss: 0.10600124299526215] [G loss: 7.624447822570801]\n",
      "[Epoch 38/200] [Batch 1/2] [D loss: 0.38195884227752686] [G loss: 14.593605995178223]\n",
      "[Epoch 39/200] [Batch 0/2] [D loss: 0.09492981433868408] [G loss: 8.002689361572266]\n",
      "[Epoch 39/200] [Batch 1/2] [D loss: 0.21291811764240265] [G loss: 11.764968872070312]\n",
      "[Epoch 40/200] [Batch 0/2] [D loss: 0.07126300781965256] [G loss: 7.464733123779297]\n",
      "[Epoch 40/200] [Batch 1/2] [D loss: 0.0925283282995224] [G loss: 12.687066078186035]\n",
      "[Epoch 41/200] [Batch 0/2] [D loss: 0.05499080568552017] [G loss: 7.726321697235107]\n",
      "[Epoch 41/200] [Batch 1/2] [D loss: 0.11522559821605682] [G loss: 10.943626403808594]\n",
      "[Epoch 42/200] [Batch 0/2] [D loss: 0.12680765986442566] [G loss: 7.988387107849121]\n",
      "[Epoch 42/200] [Batch 1/2] [D loss: 0.04851362109184265] [G loss: 12.628337860107422]\n",
      "[Epoch 43/200] [Batch 0/2] [D loss: 0.21815891563892365] [G loss: 8.19698429107666]\n",
      "[Epoch 43/200] [Batch 1/2] [D loss: 0.07164870947599411] [G loss: 13.963659286499023]\n",
      "[Epoch 44/200] [Batch 0/2] [D loss: 0.14439335465431213] [G loss: 9.227747917175293]\n",
      "[Epoch 44/200] [Batch 1/2] [D loss: 0.08816616237163544] [G loss: 14.353705406188965]\n",
      "[Epoch 45/200] [Batch 0/2] [D loss: 0.07857690751552582] [G loss: 8.172221183776855]\n",
      "[Epoch 45/200] [Batch 1/2] [D loss: 0.11255498975515366] [G loss: 10.417245864868164]\n",
      "[Epoch 46/200] [Batch 0/2] [D loss: 0.07996120303869247] [G loss: 8.429518699645996]\n",
      "[Epoch 46/200] [Batch 1/2] [D loss: 0.07666230201721191] [G loss: 13.359624862670898]\n",
      "[Epoch 47/200] [Batch 0/2] [D loss: 0.05623669922351837] [G loss: 8.415023803710938]\n",
      "[Epoch 47/200] [Batch 1/2] [D loss: 0.07819656282663345] [G loss: 10.004837036132812]\n",
      "[Epoch 48/200] [Batch 0/2] [D loss: 0.034621939063072205] [G loss: 8.057350158691406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48/200] [Batch 1/2] [D loss: 0.23168474435806274] [G loss: 13.307082176208496]\n",
      "[Epoch 49/200] [Batch 0/2] [D loss: 0.051124073565006256] [G loss: 7.755587577819824]\n",
      "[Epoch 49/200] [Batch 1/2] [D loss: 0.06720001995563507] [G loss: 14.554376602172852]\n",
      "[Epoch 50/200] [Batch 0/2] [D loss: 0.05883710831403732] [G loss: 7.334502696990967]\n",
      "[Epoch 50/200] [Batch 1/2] [D loss: 0.03935374319553375] [G loss: 14.061630249023438]\n",
      "[Epoch 51/200] [Batch 0/2] [D loss: 0.040714412927627563] [G loss: 8.403129577636719]\n",
      "[Epoch 51/200] [Batch 1/2] [D loss: 0.06321173906326294] [G loss: 14.143854141235352]\n",
      "[Epoch 52/200] [Batch 0/2] [D loss: 0.09086836874485016] [G loss: 9.023553848266602]\n",
      "[Epoch 52/200] [Batch 1/2] [D loss: 0.3188236355781555] [G loss: 36.29304885864258]\n",
      "[Epoch 53/200] [Batch 0/2] [D loss: 0.16816116869449615] [G loss: 8.03206729888916]\n",
      "[Epoch 53/200] [Batch 1/2] [D loss: 0.0715629830956459] [G loss: 18.70524787902832]\n",
      "[Epoch 54/200] [Batch 0/2] [D loss: 0.050013456493616104] [G loss: 7.735683441162109]\n",
      "[Epoch 54/200] [Batch 1/2] [D loss: 0.10259822756052017] [G loss: 12.572097778320312]\n",
      "[Epoch 55/200] [Batch 0/2] [D loss: 0.028209887444972992] [G loss: 7.82998514175415]\n",
      "[Epoch 55/200] [Batch 1/2] [D loss: 0.050782304257154465] [G loss: 15.13853931427002]\n",
      "[Epoch 56/200] [Batch 0/2] [D loss: 0.04865176975727081] [G loss: 7.081324577331543]\n",
      "[Epoch 56/200] [Batch 1/2] [D loss: 0.11761890351772308] [G loss: 12.893475532531738]\n",
      "[Epoch 57/200] [Batch 0/2] [D loss: 0.10665146261453629] [G loss: 7.635234832763672]\n",
      "[Epoch 57/200] [Batch 1/2] [D loss: 0.10201839357614517] [G loss: 13.253799438476562]\n",
      "[Epoch 58/200] [Batch 0/2] [D loss: 0.06427896022796631] [G loss: 6.934910774230957]\n",
      "[Epoch 58/200] [Batch 1/2] [D loss: 0.12504388391971588] [G loss: 11.431729316711426]\n",
      "[Epoch 59/200] [Batch 0/2] [D loss: 0.34107184410095215] [G loss: 7.902866363525391]\n",
      "[Epoch 59/200] [Batch 1/2] [D loss: 0.11476276814937592] [G loss: 17.786266326904297]\n",
      "[Epoch 60/200] [Batch 0/2] [D loss: 0.34807461500167847] [G loss: 6.7714338302612305]\n",
      "[Epoch 60/200] [Batch 1/2] [D loss: 0.21320071816444397] [G loss: 12.23455810546875]\n",
      "[Epoch 61/200] [Batch 0/2] [D loss: 0.2152310609817505] [G loss: 7.509918212890625]\n",
      "[Epoch 61/200] [Batch 1/2] [D loss: 0.08316099643707275] [G loss: 14.311729431152344]\n",
      "[Epoch 62/200] [Batch 0/2] [D loss: 0.09254220128059387] [G loss: 7.205165863037109]\n",
      "[Epoch 62/200] [Batch 1/2] [D loss: 0.19973504543304443] [G loss: 10.708730697631836]\n",
      "[Epoch 63/200] [Batch 0/2] [D loss: 0.15694211423397064] [G loss: 6.8949384689331055]\n",
      "[Epoch 63/200] [Batch 1/2] [D loss: 0.08319850265979767] [G loss: 16.53537368774414]\n",
      "[Epoch 64/200] [Batch 0/2] [D loss: 0.09560184925794601] [G loss: 6.907664775848389]\n",
      "[Epoch 64/200] [Batch 1/2] [D loss: 0.06140918284654617] [G loss: 14.990753173828125]\n",
      "[Epoch 65/200] [Batch 0/2] [D loss: 0.112397201359272] [G loss: 7.555325984954834]\n",
      "[Epoch 65/200] [Batch 1/2] [D loss: 0.05319703742861748] [G loss: 12.079853057861328]\n",
      "[Epoch 66/200] [Batch 0/2] [D loss: 0.160955011844635] [G loss: 6.70662784576416]\n",
      "[Epoch 66/200] [Batch 1/2] [D loss: 0.01645466312766075] [G loss: 9.420123100280762]\n",
      "[Epoch 67/200] [Batch 0/2] [D loss: 0.07641714066267014] [G loss: 6.801673412322998]\n",
      "[Epoch 67/200] [Batch 1/2] [D loss: 0.07773583382368088] [G loss: 13.665865898132324]\n",
      "[Epoch 68/200] [Batch 0/2] [D loss: 0.03688409551978111] [G loss: 7.077670097351074]\n",
      "[Epoch 68/200] [Batch 1/2] [D loss: 0.1040508970618248] [G loss: 10.536552429199219]\n",
      "[Epoch 69/200] [Batch 0/2] [D loss: 0.06385798007249832] [G loss: 7.589385032653809]\n",
      "[Epoch 69/200] [Batch 1/2] [D loss: 0.013383980840444565] [G loss: 12.366024017333984]\n",
      "[Epoch 70/200] [Batch 0/2] [D loss: 0.039415761828422546] [G loss: 7.561196804046631]\n",
      "[Epoch 70/200] [Batch 1/2] [D loss: 0.059518080204725266] [G loss: 17.035764694213867]\n",
      "[Epoch 71/200] [Batch 0/2] [D loss: 0.05375774949789047] [G loss: 6.643905162811279]\n",
      "[Epoch 71/200] [Batch 1/2] [D loss: 0.061451975256204605] [G loss: 12.785468101501465]\n",
      "[Epoch 72/200] [Batch 0/2] [D loss: 0.06062757596373558] [G loss: 7.67575216293335]\n",
      "[Epoch 72/200] [Batch 1/2] [D loss: 0.018480967730283737] [G loss: 12.609511375427246]\n",
      "[Epoch 73/200] [Batch 0/2] [D loss: 0.030401118099689484] [G loss: 6.952472686767578]\n",
      "[Epoch 73/200] [Batch 1/2] [D loss: 0.037367917597293854] [G loss: 12.453075408935547]\n",
      "[Epoch 74/200] [Batch 0/2] [D loss: 0.04193982109427452] [G loss: 6.801173210144043]\n",
      "[Epoch 74/200] [Batch 1/2] [D loss: 0.0740506500005722] [G loss: 11.377279281616211]\n",
      "[Epoch 75/200] [Batch 0/2] [D loss: 0.029955854639410973] [G loss: 6.875452041625977]\n",
      "[Epoch 75/200] [Batch 1/2] [D loss: 0.0807894915342331] [G loss: 12.073381423950195]\n",
      "[Epoch 76/200] [Batch 0/2] [D loss: 0.025306206196546555] [G loss: 6.803004741668701]\n",
      "[Epoch 76/200] [Batch 1/2] [D loss: 0.019827276468276978] [G loss: 11.264938354492188]\n",
      "[Epoch 77/200] [Batch 0/2] [D loss: 0.02247072011232376] [G loss: 7.21305513381958]\n",
      "[Epoch 77/200] [Batch 1/2] [D loss: 0.06452693045139313] [G loss: 16.516281127929688]\n",
      "[Epoch 78/200] [Batch 0/2] [D loss: 0.024941328912973404] [G loss: 6.9752655029296875]\n",
      "[Epoch 78/200] [Batch 1/2] [D loss: 0.10831598937511444] [G loss: 10.776375770568848]\n",
      "[Epoch 79/200] [Batch 0/2] [D loss: 0.09326329082250595] [G loss: 6.5689592361450195]\n",
      "[Epoch 79/200] [Batch 1/2] [D loss: 0.033331822603940964] [G loss: 12.758707046508789]\n",
      "[Epoch 80/200] [Batch 0/2] [D loss: 0.053643159568309784] [G loss: 6.748512268066406]\n",
      "[Epoch 80/200] [Batch 1/2] [D loss: 0.028271250426769257] [G loss: 11.67583179473877]\n",
      "[Epoch 81/200] [Batch 0/2] [D loss: 0.03132162243127823] [G loss: 6.78060245513916]\n",
      "[Epoch 81/200] [Batch 1/2] [D loss: 0.05398518592119217] [G loss: 10.654328346252441]\n",
      "[Epoch 82/200] [Batch 0/2] [D loss: 0.06606397032737732] [G loss: 8.778199195861816]\n",
      "[Epoch 82/200] [Batch 1/2] [D loss: 0.18611924350261688] [G loss: 34.53650665283203]\n",
      "[Epoch 83/200] [Batch 0/2] [D loss: 0.11708517372608185] [G loss: 7.140976905822754]\n",
      "[Epoch 83/200] [Batch 1/2] [D loss: 0.0299433171749115] [G loss: 10.068757057189941]\n",
      "[Epoch 84/200] [Batch 0/2] [D loss: 0.041262004524469376] [G loss: 7.562199115753174]\n",
      "[Epoch 84/200] [Batch 1/2] [D loss: 0.0167391374707222] [G loss: 9.606922149658203]\n",
      "[Epoch 85/200] [Batch 0/2] [D loss: 0.018607325851917267] [G loss: 6.875848770141602]\n",
      "[Epoch 85/200] [Batch 1/2] [D loss: 0.05266094580292702] [G loss: 11.576147079467773]\n",
      "[Epoch 86/200] [Batch 0/2] [D loss: 0.014296851120889187] [G loss: 7.060712814331055]\n",
      "[Epoch 86/200] [Batch 1/2] [D loss: 0.017445391044020653] [G loss: 10.191298484802246]\n",
      "[Epoch 87/200] [Batch 0/2] [D loss: 0.00881703570485115] [G loss: 7.137655735015869]\n",
      "[Epoch 87/200] [Batch 1/2] [D loss: 0.014373483136296272] [G loss: 8.679344177246094]\n",
      "[Epoch 88/200] [Batch 0/2] [D loss: 0.0067465417087078094] [G loss: 6.907290458679199]\n",
      "[Epoch 88/200] [Batch 1/2] [D loss: 0.006647038273513317] [G loss: 8.000492095947266]\n",
      "[Epoch 89/200] [Batch 0/2] [D loss: 0.004750756546854973] [G loss: 6.7302632331848145]\n",
      "[Epoch 89/200] [Batch 1/2] [D loss: 0.01623040810227394] [G loss: 15.209383010864258]\n",
      "[Epoch 90/200] [Batch 0/2] [D loss: 0.008156108669936657] [G loss: 6.733632564544678]\n",
      "[Epoch 90/200] [Batch 1/2] [D loss: 0.0132217425853014] [G loss: 9.794319152832031]\n",
      "[Epoch 91/200] [Batch 0/2] [D loss: 0.011958236806094646] [G loss: 7.068220138549805]\n",
      "[Epoch 91/200] [Batch 1/2] [D loss: 0.057376328855752945] [G loss: 11.958621978759766]\n",
      "[Epoch 92/200] [Batch 0/2] [D loss: 0.006973265204578638] [G loss: 6.800013065338135]\n",
      "[Epoch 92/200] [Batch 1/2] [D loss: 0.0509052500128746] [G loss: 13.5758695602417]\n",
      "[Epoch 93/200] [Batch 0/2] [D loss: 0.00993881095200777] [G loss: 6.850271224975586]\n",
      "[Epoch 93/200] [Batch 1/2] [D loss: 0.03859752416610718] [G loss: 12.383454322814941]\n",
      "[Epoch 94/200] [Batch 0/2] [D loss: 0.008610357530415058] [G loss: 6.6276702880859375]\n",
      "[Epoch 94/200] [Batch 1/2] [D loss: 0.012082841247320175] [G loss: 12.258987426757812]\n",
      "[Epoch 95/200] [Batch 0/2] [D loss: 0.005489069502800703] [G loss: 6.960147857666016]\n",
      "[Epoch 95/200] [Batch 1/2] [D loss: 0.006424149498343468] [G loss: 11.623454093933105]\n",
      "[Epoch 96/200] [Batch 0/2] [D loss: 0.0069574592635035515] [G loss: 6.713700294494629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 96/200] [Batch 1/2] [D loss: 0.020789634436368942] [G loss: 10.261397361755371]\n",
      "[Epoch 97/200] [Batch 0/2] [D loss: 0.009716411121189594] [G loss: 6.574942111968994]\n",
      "[Epoch 97/200] [Batch 1/2] [D loss: 0.011857820674777031] [G loss: 9.08332633972168]\n",
      "[Epoch 98/200] [Batch 0/2] [D loss: 0.009121004492044449] [G loss: 6.798091411590576]\n",
      "[Epoch 98/200] [Batch 1/2] [D loss: 0.08070866763591766] [G loss: 12.27329158782959]\n",
      "[Epoch 99/200] [Batch 0/2] [D loss: 0.06415308266878128] [G loss: 7.053114414215088]\n",
      "[Epoch 99/200] [Batch 1/2] [D loss: 0.10944517701864243] [G loss: 13.944321632385254]\n",
      "[Epoch 100/200] [Batch 0/2] [D loss: 0.08620849251747131] [G loss: 6.1417388916015625]\n",
      "[Epoch 100/200] [Batch 1/2] [D loss: 0.2049182802438736] [G loss: 10.09762191772461]\n",
      "[Epoch 101/200] [Batch 0/2] [D loss: 0.23692889511585236] [G loss: 7.245144367218018]\n",
      "[Epoch 101/200] [Batch 1/2] [D loss: 0.050322890281677246] [G loss: 9.921953201293945]\n",
      "[Epoch 102/200] [Batch 0/2] [D loss: 0.09135754406452179] [G loss: 6.672820091247559]\n",
      "[Epoch 102/200] [Batch 1/2] [D loss: 0.052487581968307495] [G loss: 11.670671463012695]\n",
      "[Epoch 103/200] [Batch 0/2] [D loss: 0.10161548107862473] [G loss: 6.73777961730957]\n",
      "[Epoch 103/200] [Batch 1/2] [D loss: 0.028884591534733772] [G loss: 16.672016143798828]\n",
      "[Epoch 104/200] [Batch 0/2] [D loss: 0.020640190690755844] [G loss: 8.934854507446289]\n",
      "[Epoch 104/200] [Batch 1/2] [D loss: 0.13372841477394104] [G loss: 32.013816833496094]\n",
      "[Epoch 105/200] [Batch 0/2] [D loss: 0.032732851803302765] [G loss: 6.792093276977539]\n",
      "[Epoch 105/200] [Batch 1/2] [D loss: 0.12600573897361755] [G loss: 9.812116622924805]\n",
      "[Epoch 106/200] [Batch 0/2] [D loss: 0.12128465622663498] [G loss: 7.342362880706787]\n",
      "[Epoch 106/200] [Batch 1/2] [D loss: 0.04890548065304756] [G loss: 9.535490036010742]\n",
      "[Epoch 107/200] [Batch 0/2] [D loss: 0.06210898980498314] [G loss: 6.181500434875488]\n",
      "[Epoch 107/200] [Batch 1/2] [D loss: 0.03011125698685646] [G loss: 9.645500183105469]\n",
      "[Epoch 108/200] [Batch 0/2] [D loss: 0.03445018082857132] [G loss: 6.586280345916748]\n",
      "[Epoch 108/200] [Batch 1/2] [D loss: 0.023437388241291046] [G loss: 9.872042655944824]\n",
      "[Epoch 109/200] [Batch 0/2] [D loss: 0.06410377472639084] [G loss: 6.7023234367370605]\n",
      "[Epoch 109/200] [Batch 1/2] [D loss: 0.03503856435418129] [G loss: 8.672121047973633]\n",
      "[Epoch 110/200] [Batch 0/2] [D loss: 0.06342889368534088] [G loss: 9.254716873168945]\n",
      "[Epoch 110/200] [Batch 1/2] [D loss: 0.22226960957050323] [G loss: 30.218873977661133]\n",
      "[Epoch 111/200] [Batch 0/2] [D loss: 0.06641848385334015] [G loss: 7.372803211212158]\n",
      "[Epoch 111/200] [Batch 1/2] [D loss: 0.15431290864944458] [G loss: 10.348483085632324]\n",
      "[Epoch 112/200] [Batch 0/2] [D loss: 0.05987437441945076] [G loss: 6.731048583984375]\n",
      "[Epoch 112/200] [Batch 1/2] [D loss: 0.05090571939945221] [G loss: 8.87590217590332]\n",
      "[Epoch 113/200] [Batch 0/2] [D loss: 0.04851420968770981] [G loss: 7.195242881774902]\n",
      "[Epoch 113/200] [Batch 1/2] [D loss: 0.04218770191073418] [G loss: 13.823657989501953]\n",
      "[Epoch 114/200] [Batch 0/2] [D loss: 0.04405561834573746] [G loss: 6.670081615447998]\n",
      "[Epoch 114/200] [Batch 1/2] [D loss: 0.021622829139232635] [G loss: 11.584308624267578]\n",
      "[Epoch 115/200] [Batch 0/2] [D loss: 0.018883926793932915] [G loss: 6.732663154602051]\n",
      "[Epoch 115/200] [Batch 1/2] [D loss: 0.014335138723254204] [G loss: 10.357057571411133]\n",
      "[Epoch 116/200] [Batch 0/2] [D loss: 0.009665228426456451] [G loss: 6.6346564292907715]\n",
      "[Epoch 116/200] [Batch 1/2] [D loss: 0.011410871520638466] [G loss: 11.99185848236084]\n",
      "[Epoch 117/200] [Batch 0/2] [D loss: 0.005581998731940985] [G loss: 6.6179399490356445]\n",
      "[Epoch 117/200] [Batch 1/2] [D loss: 0.010670655407011509] [G loss: 9.832247734069824]\n",
      "[Epoch 118/200] [Batch 0/2] [D loss: 0.008093860931694508] [G loss: 6.762669086456299]\n",
      "[Epoch 118/200] [Batch 1/2] [D loss: 0.006534010637551546] [G loss: 15.801477432250977]\n",
      "[Epoch 119/200] [Batch 0/2] [D loss: 0.010809889063239098] [G loss: 6.343453407287598]\n",
      "[Epoch 119/200] [Batch 1/2] [D loss: 0.016088571399450302] [G loss: 11.167985916137695]\n",
      "[Epoch 120/200] [Batch 0/2] [D loss: 0.009514469653367996] [G loss: 6.896819114685059]\n",
      "[Epoch 120/200] [Batch 1/2] [D loss: 0.011853523552417755] [G loss: 15.015326499938965]\n",
      "[Epoch 121/200] [Batch 0/2] [D loss: 0.014813685789704323] [G loss: 6.927571773529053]\n",
      "[Epoch 121/200] [Batch 1/2] [D loss: 0.015704892575740814] [G loss: 9.738412857055664]\n",
      "[Epoch 122/200] [Batch 0/2] [D loss: 0.017310231924057007] [G loss: 7.819391250610352]\n",
      "[Epoch 122/200] [Batch 1/2] [D loss: 0.18250399827957153] [G loss: 31.38466453552246]\n",
      "[Epoch 123/200] [Batch 0/2] [D loss: 0.03625316172838211] [G loss: 6.5405988693237305]\n",
      "[Epoch 123/200] [Batch 1/2] [D loss: 0.04379953444004059] [G loss: 12.935419082641602]\n",
      "[Epoch 124/200] [Batch 0/2] [D loss: 0.05395206809043884] [G loss: 6.992950439453125]\n",
      "[Epoch 124/200] [Batch 1/2] [D loss: 0.06741611659526825] [G loss: 12.193674087524414]\n",
      "[Epoch 125/200] [Batch 0/2] [D loss: 0.057625047862529755] [G loss: 6.849735260009766]\n",
      "[Epoch 125/200] [Batch 1/2] [D loss: 0.036222636699676514] [G loss: 10.57481575012207]\n",
      "[Epoch 126/200] [Batch 0/2] [D loss: 0.05606463924050331] [G loss: 8.815450668334961]\n",
      "[Epoch 126/200] [Batch 1/2] [D loss: 0.018318846821784973] [G loss: 13.401297569274902]\n",
      "[Epoch 127/200] [Batch 0/2] [D loss: 0.04319315776228905] [G loss: 8.037436485290527]\n",
      "[Epoch 127/200] [Batch 1/2] [D loss: 0.022105969488620758] [G loss: 13.056747436523438]\n",
      "[Epoch 128/200] [Batch 0/2] [D loss: 0.041353318840265274] [G loss: 8.190086364746094]\n",
      "[Epoch 128/200] [Batch 1/2] [D loss: 0.011414704844355583] [G loss: 12.157807350158691]\n",
      "[Epoch 129/200] [Batch 0/2] [D loss: 0.02856818214058876] [G loss: 8.470980644226074]\n",
      "[Epoch 129/200] [Batch 1/2] [D loss: 0.09245645999908447] [G loss: 29.722692489624023]\n",
      "[Epoch 130/200] [Batch 0/2] [D loss: 0.05651678889989853] [G loss: 8.143073081970215]\n",
      "[Epoch 130/200] [Batch 1/2] [D loss: 0.026693597435951233] [G loss: 12.730473518371582]\n",
      "[Epoch 131/200] [Batch 0/2] [D loss: 0.029901564121246338] [G loss: 7.215063095092773]\n",
      "[Epoch 131/200] [Batch 1/2] [D loss: 0.008668884634971619] [G loss: 16.467178344726562]\n",
      "[Epoch 132/200] [Batch 0/2] [D loss: 0.012430368922650814] [G loss: 6.916069030761719]\n",
      "[Epoch 132/200] [Batch 1/2] [D loss: 0.014593459665775299] [G loss: 11.427806854248047]\n",
      "[Epoch 133/200] [Batch 0/2] [D loss: 0.010853702202439308] [G loss: 7.22517204284668]\n",
      "[Epoch 133/200] [Batch 1/2] [D loss: 0.004634800832718611] [G loss: 11.365519523620605]\n",
      "[Epoch 134/200] [Batch 0/2] [D loss: 0.010835636407136917] [G loss: 6.829407691955566]\n",
      "[Epoch 134/200] [Batch 1/2] [D loss: 0.0027961074374616146] [G loss: 9.11894702911377]\n",
      "[Epoch 135/200] [Batch 0/2] [D loss: 0.004475385416299105] [G loss: 6.71491813659668]\n",
      "[Epoch 135/200] [Batch 1/2] [D loss: 0.009888151660561562] [G loss: 13.162641525268555]\n",
      "[Epoch 136/200] [Batch 0/2] [D loss: 0.0037607899866998196] [G loss: 7.189685821533203]\n",
      "[Epoch 136/200] [Batch 1/2] [D loss: 0.002696906216442585] [G loss: 11.888978958129883]\n",
      "[Epoch 137/200] [Batch 0/2] [D loss: 0.004223628900945187] [G loss: 7.284149169921875]\n",
      "[Epoch 137/200] [Batch 1/2] [D loss: 0.006789774633944035] [G loss: 16.085039138793945]\n",
      "[Epoch 138/200] [Batch 0/2] [D loss: 0.0025113262236118317] [G loss: 6.658875465393066]\n",
      "[Epoch 138/200] [Batch 1/2] [D loss: 0.022732647135853767] [G loss: 11.498443603515625]\n",
      "[Epoch 139/200] [Batch 0/2] [D loss: 0.015370655804872513] [G loss: 6.627902507781982]\n",
      "[Epoch 139/200] [Batch 1/2] [D loss: 0.015897512435913086] [G loss: 10.560635566711426]\n",
      "[Epoch 140/200] [Batch 0/2] [D loss: 0.014408662915229797] [G loss: 6.438229560852051]\n",
      "[Epoch 140/200] [Batch 1/2] [D loss: 0.008357569575309753] [G loss: 12.174308776855469]\n",
      "[Epoch 141/200] [Batch 0/2] [D loss: 0.003097936976701021] [G loss: 6.765564918518066]\n",
      "[Epoch 141/200] [Batch 1/2] [D loss: 0.012909590266644955] [G loss: 10.873138427734375]\n",
      "[Epoch 142/200] [Batch 0/2] [D loss: 0.002701100194826722] [G loss: 7.617809295654297]\n",
      "[Epoch 142/200] [Batch 1/2] [D loss: 0.026274986565113068] [G loss: 14.508516311645508]\n",
      "[Epoch 143/200] [Batch 0/2] [D loss: 0.0038899986539036036] [G loss: 6.703248977661133]\n",
      "[Epoch 143/200] [Batch 1/2] [D loss: 0.007367598824203014] [G loss: 13.163999557495117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 144/200] [Batch 0/2] [D loss: 0.004576793871819973] [G loss: 7.202338218688965]\n",
      "[Epoch 144/200] [Batch 1/2] [D loss: 0.004687233362346888] [G loss: 11.968530654907227]\n",
      "[Epoch 145/200] [Batch 0/2] [D loss: 0.00337788974866271] [G loss: 6.568627834320068]\n",
      "[Epoch 145/200] [Batch 1/2] [D loss: 0.006350407842546701] [G loss: 9.677936553955078]\n",
      "[Epoch 146/200] [Batch 0/2] [D loss: 0.005940022878348827] [G loss: 6.705404281616211]\n",
      "[Epoch 146/200] [Batch 1/2] [D loss: 0.003294383641332388] [G loss: 15.154261589050293]\n",
      "[Epoch 147/200] [Batch 0/2] [D loss: 0.0036590155214071274] [G loss: 6.344959735870361]\n",
      "[Epoch 147/200] [Batch 1/2] [D loss: 0.0062318602576851845] [G loss: 12.458914756774902]\n",
      "[Epoch 148/200] [Batch 0/2] [D loss: 0.004316940903663635] [G loss: 8.857450485229492]\n",
      "[Epoch 148/200] [Batch 1/2] [D loss: 0.07059997320175171] [G loss: 30.944690704345703]\n",
      "[Epoch 149/200] [Batch 0/2] [D loss: 0.01243998110294342] [G loss: 8.499937057495117]\n",
      "[Epoch 149/200] [Batch 1/2] [D loss: 0.047315724194049835] [G loss: 11.223990440368652]\n",
      "[Epoch 150/200] [Batch 0/2] [D loss: 0.06573668122291565] [G loss: 7.900099754333496]\n",
      "[Epoch 150/200] [Batch 1/2] [D loss: 0.07458599656820297] [G loss: 26.692672729492188]\n",
      "[Epoch 151/200] [Batch 0/2] [D loss: 0.020056530833244324] [G loss: 7.488742828369141]\n",
      "[Epoch 151/200] [Batch 1/2] [D loss: 0.2344616800546646] [G loss: 9.307380676269531]\n",
      "[Epoch 152/200] [Batch 0/2] [D loss: 0.09811490029096603] [G loss: 7.17500638961792]\n",
      "[Epoch 152/200] [Batch 1/2] [D loss: 0.08116961270570755] [G loss: 11.221948623657227]\n",
      "[Epoch 153/200] [Batch 0/2] [D loss: 0.038358159363269806] [G loss: 6.456821441650391]\n",
      "[Epoch 153/200] [Batch 1/2] [D loss: 0.03188470005989075] [G loss: 11.03278923034668]\n",
      "[Epoch 154/200] [Batch 0/2] [D loss: 0.05263078212738037] [G loss: 6.755093574523926]\n",
      "[Epoch 154/200] [Batch 1/2] [D loss: 0.04274017736315727] [G loss: 9.499463081359863]\n",
      "[Epoch 155/200] [Batch 0/2] [D loss: 0.04785327985882759] [G loss: 8.199629783630371]\n",
      "[Epoch 155/200] [Batch 1/2] [D loss: 0.06147809326648712] [G loss: 26.37005043029785]\n",
      "[Epoch 156/200] [Batch 0/2] [D loss: 0.050598058849573135] [G loss: 6.80040979385376]\n",
      "[Epoch 156/200] [Batch 1/2] [D loss: 0.02799006924033165] [G loss: 8.880781173706055]\n",
      "[Epoch 157/200] [Batch 0/2] [D loss: 0.056199491024017334] [G loss: 6.983160018920898]\n",
      "[Epoch 157/200] [Batch 1/2] [D loss: 0.028387241065502167] [G loss: 13.273714065551758]\n",
      "[Epoch 158/200] [Batch 0/2] [D loss: 0.03221701830625534] [G loss: 6.733397006988525]\n",
      "[Epoch 158/200] [Batch 1/2] [D loss: 0.040188778191804886] [G loss: 10.34019660949707]\n",
      "[Epoch 159/200] [Batch 0/2] [D loss: 0.026392720639705658] [G loss: 7.044979572296143]\n",
      "[Epoch 159/200] [Batch 1/2] [D loss: 0.023502854630351067] [G loss: 9.790428161621094]\n",
      "[Epoch 160/200] [Batch 0/2] [D loss: 0.016054321080446243] [G loss: 6.966513633728027]\n",
      "[Epoch 160/200] [Batch 1/2] [D loss: 0.012998858466744423] [G loss: 8.903380393981934]\n",
      "[Epoch 161/200] [Batch 0/2] [D loss: 0.008758766576647758] [G loss: 7.163287162780762]\n",
      "[Epoch 161/200] [Batch 1/2] [D loss: 0.012506209313869476] [G loss: 13.370762825012207]\n",
      "[Epoch 162/200] [Batch 0/2] [D loss: 0.004337340127676725] [G loss: 6.6625237464904785]\n",
      "[Epoch 162/200] [Batch 1/2] [D loss: 0.042800892144441605] [G loss: 10.243103981018066]\n",
      "[Epoch 163/200] [Batch 0/2] [D loss: 0.020982565358281136] [G loss: 6.718380928039551]\n",
      "[Epoch 163/200] [Batch 1/2] [D loss: 0.02347613126039505] [G loss: 11.654192924499512]\n",
      "[Epoch 164/200] [Batch 0/2] [D loss: 0.017939185723662376] [G loss: 6.159374237060547]\n",
      "[Epoch 164/200] [Batch 1/2] [D loss: 0.018526382744312286] [G loss: 8.863905906677246]\n",
      "[Epoch 165/200] [Batch 0/2] [D loss: 0.01110403798520565] [G loss: 7.178074836730957]\n",
      "[Epoch 165/200] [Batch 1/2] [D loss: 0.04418228939175606] [G loss: 15.787677764892578]\n",
      "[Epoch 166/200] [Batch 0/2] [D loss: 0.04996316134929657] [G loss: 6.315216064453125]\n",
      "[Epoch 166/200] [Batch 1/2] [D loss: 0.027320101857185364] [G loss: 10.244182586669922]\n",
      "[Epoch 167/200] [Batch 0/2] [D loss: 0.0480472557246685] [G loss: 6.676977157592773]\n",
      "[Epoch 167/200] [Batch 1/2] [D loss: 0.02351842075586319] [G loss: 9.0366849899292]\n",
      "[Epoch 168/200] [Batch 0/2] [D loss: 0.04565155878663063] [G loss: 6.4737091064453125]\n",
      "[Epoch 168/200] [Batch 1/2] [D loss: 0.012567784637212753] [G loss: 10.105055809020996]\n",
      "[Epoch 169/200] [Batch 0/2] [D loss: 0.04360967501997948] [G loss: 7.160755157470703]\n",
      "[Epoch 169/200] [Batch 1/2] [D loss: 0.009938625618815422] [G loss: 14.659693717956543]\n",
      "[Epoch 170/200] [Batch 0/2] [D loss: 0.020905286073684692] [G loss: 6.876081943511963]\n",
      "[Epoch 170/200] [Batch 1/2] [D loss: 0.00878047663718462] [G loss: 13.440823554992676]\n",
      "[Epoch 171/200] [Batch 0/2] [D loss: 0.014655809849500656] [G loss: 6.641524791717529]\n",
      "[Epoch 171/200] [Batch 1/2] [D loss: 0.0160912424325943] [G loss: 9.365638732910156]\n",
      "[Epoch 172/200] [Batch 0/2] [D loss: 0.012723835185170174] [G loss: 6.584659576416016]\n",
      "[Epoch 172/200] [Batch 1/2] [D loss: 0.0059063732624053955] [G loss: 9.37356185913086]\n",
      "[Epoch 173/200] [Batch 0/2] [D loss: 0.006915184669196606] [G loss: 6.4085564613342285]\n",
      "[Epoch 173/200] [Batch 1/2] [D loss: 0.007171268574893475] [G loss: 8.502506256103516]\n",
      "[Epoch 174/200] [Batch 0/2] [D loss: 0.0035142048727720976] [G loss: 6.377432823181152]\n",
      "[Epoch 174/200] [Batch 1/2] [D loss: 0.007958120666444302] [G loss: 8.355755805969238]\n",
      "[Epoch 175/200] [Batch 0/2] [D loss: 0.004051733762025833] [G loss: 7.205604553222656]\n",
      "[Epoch 175/200] [Batch 1/2] [D loss: 0.08145242184400558] [G loss: 16.209814071655273]\n",
      "[Epoch 176/200] [Batch 0/2] [D loss: 0.0416199266910553] [G loss: 7.580166816711426]\n",
      "[Epoch 176/200] [Batch 1/2] [D loss: 0.07195062935352325] [G loss: 16.27482795715332]\n",
      "[Epoch 177/200] [Batch 0/2] [D loss: 0.03430469334125519] [G loss: 6.3868255615234375]\n",
      "[Epoch 177/200] [Batch 1/2] [D loss: 0.011866901069879532] [G loss: 8.382814407348633]\n",
      "[Epoch 178/200] [Batch 0/2] [D loss: 0.044206831604242325] [G loss: 6.696201324462891]\n",
      "[Epoch 178/200] [Batch 1/2] [D loss: 0.015092914924025536] [G loss: 7.596042633056641]\n",
      "[Epoch 179/200] [Batch 0/2] [D loss: 0.011541163548827171] [G loss: 6.170704364776611]\n",
      "[Epoch 179/200] [Batch 1/2] [D loss: 0.013182326219975948] [G loss: 11.828702926635742]\n",
      "[Epoch 180/200] [Batch 0/2] [D loss: 0.011950526386499405] [G loss: 6.919189453125]\n",
      "[Epoch 180/200] [Batch 1/2] [D loss: 0.011731714941561222] [G loss: 9.592536926269531]\n",
      "[Epoch 181/200] [Batch 0/2] [D loss: 0.007108684629201889] [G loss: 7.5037760734558105]\n",
      "[Epoch 181/200] [Batch 1/2] [D loss: 0.03203710541129112] [G loss: 28.03150177001953]\n",
      "[Epoch 182/200] [Batch 0/2] [D loss: 0.009957308880984783] [G loss: 7.210939407348633]\n",
      "[Epoch 182/200] [Batch 1/2] [D loss: 0.0271946731954813] [G loss: 11.96946907043457]\n",
      "[Epoch 183/200] [Batch 0/2] [D loss: 0.02836558222770691] [G loss: 7.718632221221924]\n",
      "[Epoch 183/200] [Batch 1/2] [D loss: 0.022255420684814453] [G loss: 11.863149642944336]\n",
      "[Epoch 184/200] [Batch 0/2] [D loss: 0.009122478775680065] [G loss: 7.012479305267334]\n",
      "[Epoch 184/200] [Batch 1/2] [D loss: 0.030479418113827705] [G loss: 25.866689682006836]\n",
      "[Epoch 185/200] [Batch 0/2] [D loss: 0.009480498731136322] [G loss: 6.428750514984131]\n",
      "[Epoch 185/200] [Batch 1/2] [D loss: 0.010526807978749275] [G loss: 11.169260025024414]\n",
      "[Epoch 186/200] [Batch 0/2] [D loss: 0.0044123511761426926] [G loss: 7.172883033752441]\n",
      "[Epoch 186/200] [Batch 1/2] [D loss: 0.05175375938415527] [G loss: 9.193734169006348]\n",
      "[Epoch 187/200] [Batch 0/2] [D loss: 0.01212339848279953] [G loss: 6.969983100891113]\n",
      "[Epoch 187/200] [Batch 1/2] [D loss: 0.015947557985782623] [G loss: 10.282032012939453]\n",
      "[Epoch 188/200] [Batch 0/2] [D loss: 0.007420138455927372] [G loss: 6.781879425048828]\n",
      "[Epoch 188/200] [Batch 1/2] [D loss: 0.009582793340086937] [G loss: 7.896656036376953]\n",
      "[Epoch 189/200] [Batch 0/2] [D loss: 0.006473518908023834] [G loss: 6.4755730628967285]\n",
      "[Epoch 189/200] [Batch 1/2] [D loss: 0.008067437447607517] [G loss: 10.128473281860352]\n",
      "[Epoch 190/200] [Batch 0/2] [D loss: 0.002034254837781191] [G loss: 6.16510534286499]\n",
      "[Epoch 190/200] [Batch 1/2] [D loss: 0.006349893286824226] [G loss: 9.111926078796387]\n",
      "[Epoch 191/200] [Batch 0/2] [D loss: 0.0019729863852262497] [G loss: 6.359204292297363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 191/200] [Batch 1/2] [D loss: 0.005612166132777929] [G loss: 8.882675170898438]\n",
      "[Epoch 192/200] [Batch 0/2] [D loss: 0.002020070794969797] [G loss: 6.806778907775879]\n",
      "[Epoch 192/200] [Batch 1/2] [D loss: 0.0031106616370379925] [G loss: 9.629294395446777]\n",
      "[Epoch 193/200] [Batch 0/2] [D loss: 0.002445833059027791] [G loss: 7.338988304138184]\n",
      "[Epoch 193/200] [Batch 1/2] [D loss: 0.008347188122570515] [G loss: 8.762150764465332]\n",
      "[Epoch 194/200] [Batch 0/2] [D loss: 0.003834763076156378] [G loss: 6.399412631988525]\n",
      "[Epoch 194/200] [Batch 1/2] [D loss: 0.006034809164702892] [G loss: 11.856558799743652]\n",
      "[Epoch 195/200] [Batch 0/2] [D loss: 0.00313965929672122] [G loss: 6.4900312423706055]\n",
      "[Epoch 195/200] [Batch 1/2] [D loss: 0.003711316268891096] [G loss: 10.692161560058594]\n",
      "[Epoch 196/200] [Batch 0/2] [D loss: 0.0022548669949173927] [G loss: 7.064061164855957]\n",
      "[Epoch 196/200] [Batch 1/2] [D loss: 0.006882842630147934] [G loss: 13.37191104888916]\n",
      "[Epoch 197/200] [Batch 0/2] [D loss: 0.009667801670730114] [G loss: 6.407068252563477]\n",
      "[Epoch 197/200] [Batch 1/2] [D loss: 0.010953234508633614] [G loss: 10.4522705078125]\n",
      "[Epoch 198/200] [Batch 0/2] [D loss: 0.009695354849100113] [G loss: 6.474879264831543]\n",
      "[Epoch 198/200] [Batch 1/2] [D loss: 0.007044154219329357] [G loss: 7.29652738571167]\n",
      "[Epoch 199/200] [Batch 0/2] [D loss: 0.006315986625850201] [G loss: 7.053092002868652]\n",
      "[Epoch 199/200] [Batch 1/2] [D loss: 0.017397310584783554] [G loss: 9.000456809997559]\n"
     ]
    }
   ],
   "source": [
    "# PatchGAN Discriminator\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=4):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Loss Functions\n",
    "adversarial_loss = nn.MSELoss()\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "epochs = 200\n",
    "lr = 0.0002\n",
    "lambda_l1 = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "generator = UNetGenerator().to(device)\n",
    "discriminator = PatchDiscriminator().to(device)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    for i, (rgb, nir) in enumerate(dataloader):\n",
    "        # Move data to GPU\n",
    "        rgb, nir = rgb.to(device), nir.to(device)\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        gen_nir = generator(rgb)\n",
    "        pred_fake = discriminator(torch.cat((rgb, gen_nir), 1))\n",
    "        g_loss = adversarial_loss(pred_fake, torch.ones_like(pred_fake).to(device)) + lambda_l1 * l1_loss(gen_nir, nir)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        pred_real = discriminator(torch.cat((rgb, nir), 1))\n",
    "        loss_real = adversarial_loss(pred_real, torch.ones_like(pred_real).to(device))\n",
    "        pred_fake = discriminator(torch.cat((rgb, gen_nir.detach()), 1))\n",
    "        loss_fake = adversarial_loss(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "        d_loss = 0.5 * (loss_real + loss_fake)\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tLsqawy5Zya",
    "outputId": "56ec9aec-0775-44ff-9022-858e4e0ff750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test L1 Loss: 0.09811070933938026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09811070933938026"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def evaluate_model(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    total_l1_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for rgb, nir_true in test_dataloader:\n",
    "            rgb, nir_true = rgb.to(device), nir_true.to(device)\n",
    "            nir_pred = model(rgb)\n",
    "            l1_loss = torch.nn.L1Loss()(nir_pred, nir_true)\n",
    "            total_l1_loss += l1_loss.item()\n",
    "\n",
    "    avg_l1_loss = total_l1_loss / len(test_dataloader)\n",
    "    print(f\"Test L1 Loss: {avg_l1_loss}\")\n",
    "    return avg_l1_loss\n",
    "\n",
    "test_dir='/Users/pgt/Downloads/Sentinel train data'\n",
    "test_dataset = SEN12MSDataset(root_dir=test_dir, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "evaluate_model(generator, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Gl1OZHEOXmmX"
   },
   "outputs": [],
   "source": [
    "# Saving the generator model\n",
    "torch.save(generator.state_dict(), '/Users/pgt/Downloads/unet_generator.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igQus3b1X-1S",
    "outputId": "53685602-e142-4d3f-de7a-57f9ac4946ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/n7fnb41d6fl52ykkry9_c_3m0000gn/T/ipykernel_89560/964278186.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load('/Users/pgt/Downloads/unet_generator.pth',map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNetGenerator(\n",
       "  (encoder): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "    (4-6): 3 x Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (1-2): 2 x Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model architecture again\n",
    "generator = UNetGenerator()\n",
    "\n",
    "# Load the model weights\n",
    "generator.load_state_dict(torch.load('/Users/pgt/Downloads/unet_generator.pth',map_location=torch.device('cpu')))\n",
    "\n",
    "# Set the model to evaluation mode (important for inference)\n",
    "generator.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "I7t-MNiutfx7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def generate_nir_from_rgb(model, image_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generate NIR band from RGB image using trained model\n",
    "\n",
    "    Args:\n",
    "        model: Trained generator model\n",
    "        image_path: Path to input RGB image\n",
    "        device: Device to run inference on\n",
    "    \"\"\"\n",
    "    # Load and preprocess the RGB image\n",
    "    rgb_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    # Create transform pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Transform the image\n",
    "    rgb_tensor = transform(rgb_image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate NIR band\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_nir = model(rgb_tensor)\n",
    "\n",
    "    # Post-process the generated NIR\n",
    "    # Remove normalization and rescale back to original range\n",
    "    generated_nir = generated_nir.squeeze(0).cpu().numpy()\n",
    "    generated_nir = (generated_nir + 1) * 0.5 * 10000  # Denormalize and rescale\n",
    "\n",
    "    # Ensure values are in valid range\n",
    "    generated_nir = np.clip(generated_nir, 0, 10000)\n",
    "\n",
    "    # Convert to 16-bit unsigned integer\n",
    "    generated_nir = generated_nir.astype(np.uint16)\n",
    "\n",
    "    return generated_nir[0]  # Return single channel\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Path to your test image\n",
    "    rgb_image_path = '/Users/pgt/Downloads/test_nir.jpeg'\n",
    "\n",
    "    # Generate NIR band\n",
    "    nir_band = generate_nir_from_rgb(generator, rgb_image_path, device)\n",
    "\n",
    "    # Save the generated NIR band\n",
    "    # For visualization, we'll normalize to 8-bit range\n",
    "    nir_visualization = ((nir_band / 10000) * 255).astype(np.uint8)\n",
    "    nir_image = Image.fromarray(nir_visualization)\n",
    "    nir_image.save('generated_nir_1.png')\n",
    "\n",
    "    # Save the full-range NIR band (16-bit TIFF)\n",
    "    nir_image_16bit = Image.fromarray(nir_band)\n",
    "    nir_image_16bit.save('generated_nir_16bit_1.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
